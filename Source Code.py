# -*- coding: utf-8 -*-
"""Group5_RFM_Final_Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nXhYwXv63c6z4TK-xMexHI4P7A6KX_9I
"""

#Ket noi drive
from google.colab import drive
drive.mount('/content/gdrive')

#Them thu vien
!pip install squarify
import pandas as pd
import numpy as np

from sklearn.cluster import KMeans
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import silhouette_samples, silhouette_score
from yellowbrick.cluster import SilhouetteVisualizer

#Nhap data AdventureWorks sau khi lam sach
data = pd.read_excel('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/AdventureWorkscleaned-version-1.xlsb.xlsx')
data.head(10)

#So dong du lieu cua tung bien
data.count()

data.info()

"""Tính RFM"""

# Tinh RFM cho tung khach hang
data['OrderDateKey'] = pd.to_datetime(data['OrderDateKey'], format='%Y%m%d')
import datetime
current_date = max(data['OrderDateKey']) + datetime.timedelta(days=1)
data_RFMcal = data.groupby(['CustomerKey']).agg(
    {'OrderDateKey': lambda x: (current_date - x.max()).days,
     'SalesOrder': 'count',
     'Sales Amount': 'sum'
    }
)
data_RFMcal = data_RFMcal.round(6)
# Doi ten cot
data_RFMcal.rename(columns={'OrderDateKey':'Recency', 'SalesOrder':'Frequency', 'Sales Amount':'Monetary'},inplace=True)
data_RFMcal.head()

#Tai file data_RFMcal
data_RFMcal.to_csv('data_RFMcal.csv')
from google.colab import files
files.download('data_RFMcal.csv')

"""EDA"""

data_RFMcal.describe()

"""Tính điểm RFM"""



# Nhap file data_RFMcal
rfm_score = pd.read_csv("/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_RFMcal.csv")

rfm_score.head(5)

# Tinh diem cho tung gia tri R, F, M
rfm_score['RecencyScore']=pd.qcut(rfm_score['Recency'],5,labels=[5,4,3,2,1])
rfm_score['FrequencyScore']=pd.qcut(rfm_score['Frequency'].rank(method='first'),5,labels=[1,2,3,4,5])
rfm_score['MonetaryScore']=pd.qcut(rfm_score['Monetary'],5,labels=[1,2,3,4,5])
rfm_score.head()

# Tinh diem tong the 3 gia tri RFM
rfm_score['RFM_score']=rfm_score['RecencyScore'].astype(str)+rfm_score['FrequencyScore'].astype(str)+rfm_score['MonetaryScore'].astype(str)
rfm_score.head()

#Tải file rfm_score
#rfm_score.to_csv('rfm_score.csv')
#from google.colab import files
#files.download('rfm_score.csv')

"""

```
# This is formatted as code
```

Data transformation
"""

# Nhap file data_RFMcal
data_pretrans=pd.read_csv("/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_RFMcal (3).csv")
data_pretrans.head()

# Kiem tra phan phoi du lieu
fig, ax = plt.subplots(1, 3, figsize=(12,4))
sns.distplot(data_RFMcal['Recency'], ax = ax[0])
sns.distplot(data_RFMcal['Frequency'], ax = ax[1])
sns.distplot(data_RFMcal['Monetary'], ax = ax[2])
plt.show()

# Ham thu nghiem 4 phuong phap transform: log, square root, box-cox va cube root
from scipy import stats
def analyze_skewness(x):
    fig, ax = plt.subplots(2, 2, figsize=(5,5))
    sns.distplot(data_pretrans[x], ax=ax[0,0])
    sns.distplot(np.log(data_pretrans[x]), ax=ax[0,1])
    sns.distplot(np.sqrt(data_pretrans[x]), ax=ax[1,0])
    sns.distplot(stats.boxcox(data_pretrans[x])[0], ax=ax[1,1])
    plt.tight_layout()
    plt.show()

    print(data_pretrans[x].skew().round(2))
    print(np.log(data_pretrans[x]).skew().round(2))
    print(np.sqrt(data_pretrans[x]).skew().round(2))
    print(pd.Series(stats.boxcox(data_pretrans[x])[0]).skew().round(2))

analyze_skewness('Recency')

analyze_skewness('Frequency')

analyze_skewness('Monetary')

# Nhap file data_RFMcal
data_transformed = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_RFMcal.csv')
data_transformed.head()

# Chuan hoa
data_transformed['Recency'] = pd.Series(np.sqrt(data_transformed['Recency'])).values
data_transformed['Frequency'] = stats.boxcox(data_transformed['Frequency'])[0]
data_transformed['Monetary'] = stats.boxcox(data_transformed['Monetary'])[0]
data_transformed.head(10)

#Tai file data_transformed
data_transformed.to_csv('data_transformed.csv')
from google.colab import files
files.download('data_transformed.csv')

"""Data scaling by z-score"""

from scipy import stats
data_scaled = data_transformed.copy()
columns_to_exclude = ['CustomerKey']
data_scaled[data_transformed.columns.difference(columns_to_exclude)] = data_scaled[data_scaled.columns.difference(columns_to_exclude)].apply(stats.zscore)
data_scaled.head()

#Tai file data_scaled
data_scaled.to_csv('data_scaled.csv')
from google.colab import files
files.download('data_scaled.csv')

"""K-means"""

# Tim so cum k bang elbow method
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
data_without_customerkey = data_scaled.drop('CustomerKey', axis=1)
ssd = []
range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]
for num_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=num_clusters,init='random', random_state=42 )
    kmeans.fit(data_without_customerkey)
    ssd.append(kmeans.inertia_)
print(ssd)
# Plot
plt.figure(figsize=(10, 4))
plt.plot(range_n_clusters, ssd, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

from sklearn.metrics import silhouette_score
silhouette_scores = []

# Tạo dữ liệu mẫu
range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10,11,12]
for num_clusters in range_n_clusters:
    kmeansmodel = KMeans(n_clusters=num_clusters,init='random',random_state=42)
    kmeansmodel.fit(data_without_customerkey)
    labels = kmeansmodel.labels_
    silhouette_avg = silhouette_score(data_without_customerkey, labels)
    print(f"Chỉ số silhouette trung bình của {num_clusters}: {silhouette_avg}")

from sklearn.metrics import silhouette_score
silhouette_scores = []

# Tạo dữ liệu mẫu
range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10,11,12]
for num_clusters in range_n_clusters:
    kmeansmodel = KMeans(n_clusters=num_clusters,init='random',random_state=42)
    kmeansmodel.fit(data_without_customerkey)
    labels = kmeansmodel.fit_predict(data_without_customerkey)
    silhouette_avg = silhouette_score(data_without_customerkey, labels)
    print(f"Chỉ số silhouette trung bình của {num_clusters}: {silhouette_avg}")

# Tim so cum k bang elbow method
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
data_without_customerkey = data_scaled.drop('CustomerKey', axis=1)
ssd = []
range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]
for num_clusters in range_n_clusters:
    kmeanplus = KMeans(n_clusters=num_clusters,init='k-means++',random_state=42)
    kmeanplus.fit(data_without_customerkey)
    ssd.append(kmeanplus.inertia_)
print(ssd)
# Plot
plt.figure(figsize=(10, 4))
plt.plot(range_n_clusters, ssd, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

from sklearn.metrics import silhouette_score
from sklearn_extra.cluster import KMedoids

silhouette_scores = []

# Tạo dữ liệu mẫu
range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10,11,12]
for num_clusters in range_n_clusters:
    kmedoids_model = KMedoids(n_clusters=num_clusters,random_state=42)
    kmedoids_model.fit(data_without_customerkey)
    labels = kmedoids_model.labels_
    silhouette_avg = silhouette_score(data_without_customerkey, labels)
    print(f"Chỉ số silhouette trung bình của {num_clusters}: {silhouette_avg}")

from yellowbrick.cluster import SilhouetteVisualizer

fig, ax = plt.subplots(4, 2, figsize=(15,8))
for k in [4, 5, 6, 7, 8]:

    km = KMeans(n_clusters=k, init='random', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(k, 2)

    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(data_without_customerkey)
    visualizer.ax.set_title("")


visualizer.show()

from yellowbrick.cluster import SilhouetteVisualizer

fig, ax = plt.subplots(4, 2, figsize=(15,8))
for k in [4, 5, 6, 7, 8]:

    kmpl = KMeans(n_clusters=k, init='k-means++', random_state=42)
    q, mod = divmod(k, 2)

    visualizer = SilhouetteVisualizer(kmpl, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(data_without_customerkey)

visualizer.show()

pip install scikit-learn-extra

from yellowbrick.cluster import SilhouetteVisualizer
from sklearn_extra.cluster import KMedoids
import matplotlib.pyplot as plt

fig, ax = plt.subplots(4, 2, figsize=(15,8))
for k in [4, 5, 6, 7, 8]:

    kmm = KMedoids(n_clusters=num_clusters,random_state=42)
    kmm.fit(data_without_customerkey)
    labels = kmm.labels_
    q, mod = divmod(k, 2)

    visualizer = SilhouetteVisualizer(kmm, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(data_without_customerkey)

visualizer.show()



#Tinh RFM cho tung cum (R', F', M')
df = pd.read_csv('/content/drive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_label.csv')
rfm_mean = df.groupby('Cluster').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean','CustomerKey': 'size'})
rfm_mean = rfm_mean.rename(columns={
    'Recency': 'R\'',
    'Frequency': 'F\'',
    'Monetary': 'M\'',
    'CustomerKey': 'Nu. of Customers'})
rfm_mean.head(6)



#Tai file rfm_mean
rfm_mean.to_csv('rfm_mean.csv')
from google.colab import files
files.download('rfm_mean.csv')

#EDA
data_eda=pd.read_excel('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/AdventureWorkscleaned-version-1.xlsb.xlsx')
data_eda.head(10)

data_eda.info()

data_eda_drop=data_eda.drop(['CustomerKey','OrderDateKey','SalesOrder'], axis='columns')


data_eda_drop.describe()

data_eda_drop.mode()

-

data_eda_corr=data_eda_drop.corr()
print(data_eda_corr)

sns.scatterplot(y=data_eda['Recency'],x=df['Frequency'])

#sns.scatterplot(y=data_eda['Recency'],x=df['Monetary'])????

import seaborn as sns
sns.heatmap(data_eda_corr)

#Nhap 2 bản data AdventureWorks sau khi transformed và scaled
data = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_scaled.csv')
data_RFM=pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_RFMcal.csv')
data.head(10)

# Tạo df mới không có cột CustomerKey để chạy K-means
data_without_customerkey = data.drop(['CustomerKey'], axis=1)
data_without_customerkey.head()

# Phan cum bang Kmeans voi k=7
from sklearn.cluster import KMeans
model_KMeans = KMeans(n_clusters = 7,init='random', max_iter=500)
model_KMeans.fit(data_without_customerkey)

# Them cot nhan cum sau khi phan cum
Kmeans_clustering_scaled=data.copy()
Kmeans_clustering_scaled['Cluster'] = model_KMeans.labels_
Kmeans_clustering_scaled.head()

Kmeans_clustering_scaled=pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/Kmeans_clustering_scaled.csv')
data_RFM['Cluster']=Kmeans_clustering_scaled.Cluster
data_RFM.head()

# Tính kết quả tổng R,F,M từng cụm
df = data_RFM
rfm_sum  = df.groupby('Cluster').agg({'Recency': 'sum', 'Frequency': 'sum', 'Monetary': 'sum','CustomerKey': 'size'})
rfm_sum = rfm_sum.rename(columns={
    'Recency': 'Sum R',
    'Frequency': 'Sum F',
    'Monetary': 'Sum M',
    'CustomerKey': 'Nu. of Customers'})
rfm_sum.head(8)
#rfm_sum.to_csv('rfm_sum.csv')
#from google.colab import files
#files.download('rfm_sum.csv')

# Tính Mean R, Mean F, Mean M
df = data_RFM
rfm_mean = df.groupby('Cluster').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean','CustomerKey': 'size'})
calculate_percentage = lambda x: (x.count() / df['CustomerKey'].count()) * 100
rfm_mean['CustomerKey'] = df.groupby('Cluster')['CustomerKey'].agg(calculate_percentage)
rfm_mean = rfm_mean.rename(columns={
    'Recency': 'Mean R',
    'Frequency': 'Mean F',
    'Monetary': 'Mean M',
    'CustomerKey': 'Percentage'
})
rfm_mean['Mean R'].round(6)
rfm_mean['Mean F'].round(6)
rfm_mean['Mean M'].round(6)
rfm_mean['Percentage'].round(6)
rfm_mean.head(8)
#rfm_mean.to_csv('rfm_mean.csv')
#from google.colab import files
#files.download('rfm_mean.csv')

# Vẽ biểu đồ kiểm tra độ phân tán các cụm
from mpl_toolkits.mplot3d import Axes3D
Kmeans_clustering_scaled = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/Kmeans_clustering_scaled.csv')

# Biểu đồ không gian 3 chiều
fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')

# Vẽ điểm cho mỗi cụm
for cluster in Kmeans_clustering_scaled['Cluster'].unique():
    cluster_data = Kmeans_clustering_scaled[Kmeans_clustering_scaled['Cluster'] == cluster]
    ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'], label=f'Cluster {cluster}')

ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
ax.set_title('3D Scatter Plot of Data with 7 clusters')
ax.legend()

plt.show()



data_RFM_without_customerkey =data_RFM.drop('CustomerKey', axis=1)
def split_dataframe_by_column_value(df, column_name):

    # Group by giá trị của cột
    grouped = df.groupby(column_name)

    # Tạo một dictionary để lưu trữ các DataFrame nhỏ dựa trên giá trị của cột
    subframes = {}

    # Lặp qua từng giá trị và tạo DataFrame con cho mỗi giá trị
    for value, group in grouped:
        subframes[value] = group.copy()

    return subframes

result=split_dataframe_by_column_value(data_RFM_without_customerkey,'Cluster')
result[0].describe()

result[1].describe()

## K-MEANS++

#Nhap 2 bản data AdventureWorks sau khi transformed và scaled
data = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_scaled.csv')
data_RFM=pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_RFMcal.csv')
data.head(10)

# Tạo df mới không có cột CustomerKey để chạy K-means++
data_without_customerkey = data.drop(['CustomerKey'], axis=1)
data_without_customerkey.head()

#Phan cum bang Kmeans++
from sklearn.cluster import KMeans
import numpy as np

# Sử dụng hàm KMeans từ scikit-learn
kmeansplus = KMeans(n_clusters=7, init='k-means++', random_state=42)
kmeansplus.fit(data_without_customerkey)

# Them cot nhan cum sau khi phan cum
Kmeansplus_clustering_scaled=data.copy()
Kmeansplus_clustering_scaled['Cluster'] = kmeansplus.labels_
Kmeansplus_clustering_scaled.head()

data_RFM['Cluster']=Kmeansplus_clustering_scaled.Cluster
data_RFM.head()

df = data_RFM
rfm_sum  = df.groupby('Cluster').agg({'Recency': 'sum', 'Frequency': 'sum', 'Monetary': 'sum','CustomerKey': 'size'})
rfm_sum = rfm_sum.rename(columns={
    'Recency': 'Sum R',
    'Frequency': 'Sum F',
    'Monetary': 'Sum M',
    'CustomerKey': 'Nu. of Customers'})
rfm_sum.head(8)

df = data_RFM
rfm_mean = df.groupby('Cluster').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean','CustomerKey': 'size'})
calculate_percentage = lambda x: (x.count() / df['CustomerKey'].count()) * 100
rfm_mean['CustomerKey'] = df.groupby('Cluster')['CustomerKey'].agg(calculate_percentage)
rfm_mean = rfm_mean.rename(columns={
    'Recency': 'Mean R',
    'Frequency': 'Mean F',
    'Monetary': 'Mean M',
    'CustomerKey': 'Percentage'
})
rfm_mean.head(8)

# Vẽ biểu đồ kiểm tra độ phân tán các cụm
from mpl_toolkits.mplot3d import Axes3D
Kmeansplus_clustering_scaled = Kmeansplus_clustering_scaled

# Biểu đồ không gian 3 chiều
fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')

# Vẽ điểm cho mỗi cụm
for cluster in Kmeansplus_clustering_scaled['Cluster'].unique():
    cluster_data = Kmeansplus_clustering_scaled[Kmeansplus_clustering_scaled['Cluster'] == cluster]
    ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'], label=f'Cluster {cluster}')5

ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
ax.set_title('3D Scatter Plot of Data with 7 clusters')
ax.legend()

plt.show()

data_RFM_without_customerkey =data_RFM.drop('CustomerKey', axis=1)

def split_dataframe_by_column_value(df, column_name):

    # Group by giá trị của cột
    grouped = df.groupby(column_name)

    # Tạo một dictionary để lưu trữ các DataFrame nhỏ dựa trên giá trị của cột
    subframes = {}

    # Lặp qua từng giá trị và tạo DataFrame con cho mỗi giá trị
    for value, group in grouped:
        subframes[value] = group.copy()

    return subframes

result=split_dataframe_by_column_value(data_RFM_without_customerkey,'Cluster')
result[0].describe()

## KMEDOIDS

pip install scikit-learn-extra
!pip install squarify
import pandas as pd
import numpy as np

from sklearn.cluster import KMeans
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_scaled.csv')

from sklearn_extra.cluster import KMedoids

# Lấy tất cả các cột trừ cột "CustomerKey"
data = df.drop(columns=['CustomerKey'])

# Áp dụng thuật toán K-medoids với k=7
model_KMedoids = KMedoids(n_clusters=7, init = 'random', random_state = 42)
model_KMedoids.fit(data)

# Gắn nhãn cho cụm bởi thuật toán K-medoids
Kmedoids_clustering_scaled = df.copy()
Kmedoids_clustering_scaled['Cluster'] = model_KMedoids.labels_
Kmedoids_clustering_scaled.head()

# Tải file csv đã gắn nhãn
Kmedoids_clustering_scaled.to_csv('Kmedoids_clustering_scaled.csv')
from google.colab import files
files.download('Kmedoids_clustering_scaled.csv')

# Vẽ biểu đồ kiểm tra độ phân tán các cụm
from mpl_toolkits.mplot3d import Axes3D
data = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/Kmedoids_clustering_scaled.csv')

# Biểu đồ không gian 3 chiều
fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')

# Vẽ điểm cho mỗi cụm
for cluster in data['Cluster'].unique():
    cluster_data = data[data['Cluster'] == cluster]
    ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'], label=f'Cluster {cluster}')

ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
ax.set_title('3D Scatter Plot of Data with 7 clusters')
ax.legend()

plt.show()

data_RFM = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/data_RFMcal.csv')
Kmedoids_clustering_scaled = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/Kmedoids_clustering_scaled.csv')
data_RFM['Cluster']=Kmedoids_clustering_scaled.Cluster
data_RFM.head(10)

# Tính kết quả tổng R,F,M từng cụm Kmedoids
df = data_RFM
rfm_sum_medoids  = df.groupby('Cluster').agg({'Recency': 'sum', 'Frequency': 'sum', 'Monetary': 'sum','CustomerKey': 'size'})
rfm_sum_medoids = rfm_sum_medoids.rename(columns={
    'Recency': 'Sum R',
    'Frequency': 'Sum F',
    'Monetary': 'Sum M',
    'CustomerKey': 'Nu. of Customers'})
rfm_sum_medoids.head(10)

# Tính Mean R, Mean F, Mean M của kmedoids
df = data_RFM
rfm_mean_medoids = df.groupby('Cluster').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean','CustomerKey': 'size'})
calculate_percentage = lambda x: (x.count() / df['CustomerKey'].count()) * 100
rfm_mean_medoids['CustomerKey'] = df.groupby('Cluster')['CustomerKey'].agg(calculate_percentage)
rfm_mean_medoids = rfm_mean_medoids.rename(columns={
    'Recency': 'Mean R',
    'Frequency': 'Mean F',
    'Monetary': 'Mean M',
    'CustomerKey': 'Percentage'
})
rfm_mean_medoids.head(6)

data = data_RFM.drop('CustomerKey', axis=1)
def split_dataframe_by_column_value(df, column_name):

    # Group by giá trị của cột
    grouped = df.groupby(column_name)

    # Tạo một dictionary để lưu trữ các DataFrame nhỏ dựa trên giá trị của cột
    subframes = {}

    # Lặp qua từng giá trị và tạo DataFrame con cho mỗi giá trị
    for value, group in grouped:
        subframes[value] = group.copy()

    return subframes
result=split_dataframe_by_column_value(data,'Cluster')

result[0].describe()

## CLV

#Import necessary libraries
import pandas as pd
import matplotlib as plt

# Ket noi drive
from google.colab import drive
drive.mount('/content/gdrive')

tx_data = pd.read_excel('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/AdventureWorkscleaned-version-1.xlsb.xlsx')

#Check the shape (number of columns and rows) in the dataset
tx_data.shape

#Find out missing values
tx_data.isnull().sum(axis=0)

tx_data['OrderDateKey'] = pd.to_datetime(tx_data['OrderDateKey'], format='%Y%m%d').dt.date

# Convert the date format while keeping only the date component
tx_data['OrderDateKey'] = pd.to_datetime(tx_data['OrderDateKey']).dt.date

# Remove rows with missing CustomerKey values
tx_data = tx_data[pd.notnull(tx_data['CustomerKey'])]

# Keep records with non-negative quantity
tx_data = tx_data[(tx_data['Order Quantity'] > 0)]

# Add a new column depicting total sales
tx_data['Total_Sales'] = tx_data['Order Quantity'] * tx_data['Unit Price']

# Select necessary columns
necessary_cols = ['CustomerKey', 'OrderDateKey', 'Total_Sales']
tx_data = tx_data[necessary_cols]

# Display the first few rows of the modified dataframe
tx_data.head()

#Print records pertaining unique Customer IDs
print(tx_data['CustomerKey'].nunique())

#Check the Last order date
last_order_date = tx_data['OrderDateKey'].max()
print(last_order_date)
print("--------------------------------------")
print(tx_data[(tx_data['CustomerKey']==14355)])

pip install lifetimes

from lifetimes.plotting import *
from lifetimes.utils import *
#Built-in utility functions from lifetimes package to transform the transactional data (one row per purchase)
#into summary data (a frequency, recency, age and monetary).

lf_tx_data = summary_data_from_transaction_data(tx_data, 'CustomerKey', 'OrderDateKey', monetary_value_col='Total_Sales', observation_period_end='2019-12-31')
lf_tx_data.reset_index().head()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
#Create histogram to find out how many customers purchased item only once.
lf_tx_data['frequency'].plot(kind='hist', bins=50)
print(lf_tx_data['frequency'].describe())
print("---------------------------------------")
one_time_buyers = round(sum(lf_tx_data['frequency'] == 0)/float(len(lf_tx_data))*(100),2)
print("Percentage of customers purchase the item only once:", one_time_buyers ,"%")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# calibration_period_ends = '2019-09-22'
# 
# from lifetimes.utils import calibration_and_holdout_data
# 
# summary_cal_holdout = calibration_and_holdout_data(tx_data,
#                                                    customer_id_col = 'CustomerKey',
#                                                    datetime_col = 'OrderDateKey',
#                                                    freq = 'D', #days
#                                         calibration_period_end=calibration_period_ends,
#                                         observation_period_end='2019-12-31' )
#

#Frequency/Recency Analysis Using the BG/NBD Model
from lifetimes import ModifiedBetaGeoFitter
bgf = ModifiedBetaGeoFitter(penalizer_coef=0.0005)
bgf.fit(lf_tx_data['frequency'], lf_tx_data['recency'], lf_tx_data['T'], verbose=True)
print(bgf)

# Predicted number of transactions for the holdout period
t = 90  # Adjust this based on your business context; e.g., 90 days
lf_tx_data['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(t, lf_tx_data['frequency'], lf_tx_data['recency'], lf_tx_data['T'])

# Joining the predicted purchases with the holdout set
summary_cal_holdout = summary_cal_holdout.join(lf_tx_data[['predicted_purchases']], on='CustomerKey')

# Drop NaN values (customers not present in the training set)
summary_cal_holdout = summary_cal_holdout.dropna(subset=['predicted_purchases'])

# Calculate metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Actual vs Predicted purchases for holdout set
y_true = summary_cal_holdout['frequency_holdout']
y_pred = summary_cal_holdout['predicted_purchases']

# Calculate metrics
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases
import matplotlib.pyplot as plt
import seaborn as sns

from lifetimes.plotting import plot_period_transactions
ax = plot_period_transactions(bgf, max_frequency=3)
ax.set_yscale('log')
sns.despine();

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases
# import matplotlib.pyplot as plt
# import seaborn as sns
# 
# # Hiển thị đồ thị
# plot_calibration_purchases_vs_holdout_purchases(bgf, summary_cal_holdout)
# 
# # Đặt giới hạn cho trục x và trục y (đây là ví dụ, bạn có thể điều chỉnh theo nhu cầu của mình)
# plt.xlim(0, 1)  # Đặt giới hạn cho trục x
# plt.ylim(0, 1)  # Đặt giới hạn cho trục y
# 
# # Loại bỏ viền (spines) không cần thiết
# sns.despine()
# 
# # Hiển thị đồ thị đã chỉnh sửa
# plt.show()
#

#Customer's future transaction prediction for next 90 days

t = 90
individual = lf_tx_data.loc[11016]
bgf.predict(t, individual['frequency'], individual['recency'], individual['T'])

#OBSERVATION: Our model predicts that customer 14911’s future transaction is appx 3 in 90 days.

# Chia dữ liệu thành tập huấn luyện và tập kiểm tra
from sklearn.model_selection import train_test_split

# Tính summary data từ tập huấn luyện
lf_train_data = summary_data_from_transaction_data(train_data, 'CustomerKey', 'OrderDateKey', monetary_value_col='Total_Sales', observation_period_end='2019-09-22')

# Lựa chọn khách hàng có ít nhất một giao dịch lặp lại
shortlisted_train_customers = lf_train_data[lf_train_data['frequency'] > 0]

# Huấn luyện mô hình Gamma-Gamma trên tập huấn luyện
ggf = GammaGammaFitter(penalizer_coef=0.001)
ggf.fit(shortlisted_train_customers['frequency'], shortlisted_train_customers['monetary_value'])

# Dự đoán monetary value cho tập kiểm tra
lf_test_data = summary_data_from_transaction_data(test_data, 'CustomerKey', 'OrderDateKey', monetary_value_col='Total_Sales', observation_period_end='2019-12-31')
predicted_monetary_value_test = ggf.conditional_expected_average_profit(
    lf_test_data['frequency'],
    lf_test_data['monetary_value']
)

# Sau khi có predicted_monetary_value_test, bạn có thể tính toán CLV dựa trên frequency như bạn đã làm trong đoạn mã trước đó.

import matplotlib.pyplot as plt

# So sánh giá trị thực tế và giá trị dự đoán
plt.figure(figsize=(10, 6))

plt.scatter(lf_test_data['monetary_value'], predicted_monetary_value_test, color='blue', alpha=0.9)
plt.xlabel('Actual Monetary Value')
plt.ylabel('Predicted Monetary Value')
plt.title('Scatter plot of actual and predicted monetary value in initial dataset')
plt.grid(True)
plt.show()

#Shortlist customers who had at least one repeat purchase with the company.
shortlisted_customers = lf_tx_data[lf_tx_data['frequency']>0]
print(shortlisted_customers.head().reset_index())
print("-----------------------------------------")
print("The Number of Returning Customers are: ",len(shortlisted_customers))



#After applying Gamma-Gamma model, now we can estimate average transaction value for each customer.
print(ggf.conditional_expected_average_profit(
        lf_tx_data['frequency'],
        lf_tx_data['monetary_value']
    ).head(50))

from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, f1_score
import numpy as np

# Phân loại monetary values vào 7 bins
labels = KMeans(n_clusters=7, random_state=42).fit_predict(lf_test_data[['monetary_value']])
lf_test_data['predicted_bin'] = labels

# Phân loại dự đoán từ mô hình Gamma-Gamma vào 7 bins
bins = np.linspace(min(predicted_monetary_value_test), max(predicted_monetary_value_test), 7)  # 7 bins
lf_test_data['predicted_bin_model'] = np.digitize(predicted_monetary_value_test, bins) - 1

# Tính toán confusion matrix và F1-score
conf_matrix = confusion_matrix(lf_test_data['predicted_bin'], lf_test_data['predicted_bin_model'])
f1 = f1_score(lf_test_data['predicted_bin'], lf_test_data['predicted_bin_model'], average='weighted')

print("Confusion Matrix:")
print(conf_matrix)
print("\nF1 Score:", f1)

import matplotlib.pyplot as plt
import seaborn as sns

# Tạo heatmap cho confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=True,
            xticklabels=np.arange(7), yticklabels=np.arange(7))
plt.title('Confusion Matrix')
plt.xlabel('Predicted bins')
plt.ylabel('True bins')
plt.show()

print("\nF1 Score:", f1)

lf_tx_data['pred_txn_value'] = round(ggf.conditional_expected_average_profit(
        lf_tx_data['frequency'],
        lf_tx_data['monetary_value']), 2)
lf_tx_data.reset_index().head()

#Calculate Customer Lifetime Value
lf_tx_data['CLV'] = round(ggf.customer_lifetime_value(
    bgf, #the model to use to predict the number of future transactions
    lf_tx_data['frequency'],
    lf_tx_data['recency'],
    lf_tx_data['T'],
    lf_tx_data['monetary_value'],
    time=12, # months
    discount_rate=0.01 # monthly discount rate ~ 12.7% annually
), 2)

lf_tx_data.drop(lf_tx_data.iloc[:, 0:6], inplace=True, axis=1)

lf_tx_data.sort_values(by='CLV', ascending=False).head(10).reset_index()

label_data = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/Kmeansplus_clustering_scaled.csv', usecols=['CustomerKey','Cluster'])

# Gộp nhãn cụm vào dataframe lf_tx_data
lf_tx_data = lf_tx_data.merge(label_data, on='CustomerKey', how='left')

# Tính tổng CLV cho mỗi cụm
total_clv_per_cluster = lf_tx_data.groupby('Cluster')['CLV'].sum().reset_index()

# Hiển thị 10 cụm có tổng CLV lớn nhất
print(total_clv_per_cluster.sort_values(by='CLV', ascending=False).head(10))

import pandas as pd
import matplotlib.pyplot as plt


# Tính tổng giá trị CLV cho mỗi cụm
total_clv_per_cluster = lf_tx_data.groupby('Cluster')['CLV'].sum().reset_index()
total_clv_per_cluster_sorted = total_clv_per_cluster.sort_values(by='CLV', ascending=False).head(7)

# Tính số lượng khách hàng cho mỗi cụm dựa trên giá trị y bên phải (tối đa 2000)
max_customer_count = 2000
customer_counts_per_cluster['CustomerCount'] = customer_counts_per_cluster['CustomerCount'].apply(lambda x: min(x, max_customer_count))

# Vẽ biểu đồ
fig, ax1 = plt.subplots(figsize=(10, 6))

# Biểu đồ CLV
color = 'tab:blue'
ax1.set_xlabel('Cluster')
ax1.set_ylabel('CLV', color=color)
ax1.bar(total_clv_per_cluster_sorted['Cluster'], total_clv_per_cluster_sorted['CLV'], color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.set_ylim(-5000, 100000)  # Giới hạn trục y bên trái
import pandas as pd
import matplotlib.pyplot as plt

customer_counts_per_cluster = customer_counts_per_cluster.sort_values(by='Cluster')

# Vẽ biểu đồ
fig, ax1 = plt.subplots(figsize=(10, 6))

# Biểu đồ CLV
color = 'tab:blue'
ax1.set_xlabel('Cluster')
ax1.set_ylabel('CLV', color=color)
ax1.bar(total_clv_per_cluster_sorted['Cluster'], total_clv_per_cluster_sorted['CLV'], color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.set_ylim(-5000, 100000)  # Giới hạn trục y bên trái
ax1.axhline(y=0, color='gray', linestyle='--')

# Khởi tạo trục thứ hai cùng với trục y đầu tiên
ax2 = ax1.twinx()

color = 'tab:red'
ax2.set_ylabel('Number of Customers', color=color)
ax2.plot(customer_counts_per_cluster['Cluster'], customer_counts_per_cluster['CustomerCount'], color=color, marker='o', linestyle='-', linewidth=2)
ax2.tick_params(axis='y', labelcolor=color)
ax2.set_ylim(0, 1200)  # Giới hạn trục y bên phải

plt.title('CLV and Number of Customers by Cluster')
plt.tight_layout()
plt.show()

# Ket noi drive
from google.colab import drive
drive.mount('/content/gdrive')

#Import necessary libraries
import pandas as pd
import matplotlib as plt

data = pd.read_excel('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/AdventureWorkscleaned-version-1.xlsb.xlsx')

cohort_df = data
cohort_df.sample(8)

import datetime as dt

def first_of_month(date):
    formatted_date = dt.datetime.strptime(str(date), "%Y%m%d")
    return dt.datetime(formatted_date.year, formatted_date.month, 1)

cohort_df['invoicemonth'] = cohort_df['OrderDateKey'].apply(first_of_month)
cohort_df.head(3)

cohort_df['cohortmonth'] = cohort_df.groupby('CustomerKey')['invoicemonth'].transform('min')
cohort_df.sample(500)

def get_month_int(df, column):
    year = df[column].dt.year
    month = df[column].dt.month
    day = df[column].dt.day
    return year, month, day

invoice_year, invoice_month, invoice_day = get_month_int(cohort_df, 'invoicemonth')
cohort_year, cohort_month, cohort_day = get_month_int(cohort_df, 'cohortmonth')

cohort_month

year_diff = invoice_year - cohort_year
month_diff = invoice_month - cohort_month

cohort_df['cohortindex'] = year_diff * 12 + month_diff + 1

cohort_df.sample(10)

#Count monthly active customers from each cohort

first_cohort = cohort_df.groupby(['cohortmonth', 'cohortindex'])['CustomerKey'].nunique().reset_index()
first_cohort

cohort_df.info()



cohort_min = cohort_df['cohortmonth'].min()
cohort_min

label_data = pd.read_csv('/content/gdrive/Shareddrives/Phân tích dữ liệu cơ bản/4. FINAL/5. FILE DATA/Kmeansplus_clustering_scaled.csv', usecols=['CustomerKey','Cluster'])

# Gộp nhãn cụm vào dataframe lf_tx_data
cohort_df = cohort_df.merge(label_data, on='CustomerKey', how='left')

cohort_df.head()

import seaborn as sbn
import matplotlib.pyplot as plt

# Tính retention cho mỗi cụm khách hàng
retention_by_cluster = cohort_df.groupby(['cohortmonth', 'Cluster', 'cohortindex'])['CustomerKey'].nunique().reset_index()

# Tính toán số lượng khách hàng theo từng nhóm cụm
cluster_size = cohort_df.groupby('Cluster')['CustomerKey'].nunique()

# Tính toán retention cho mỗi nhóm cụm
retention_by_cluster['RetentionRate'] = retention_by_cluster.groupby(['Cluster', 'cohortmonth'])['CustomerKey'].apply(lambda x: x / cluster_size[x.name[0]])

# Tạo pivot table cho retention
retention_pivot = retention_by_cluster.pivot_table(index='cohortmonth', columns='Cluster', values='RetentionRate').round(2)

retention_filled = retention_pivot.fillna(0.001)  # Sử dụng 0.001 thay vì 0 để có màu sắc nhỏ hơn 0%

plt.figure(figsize=(15, 8))
plt.title('Retention Rates by 7 Cluster Over 12 Months')
sns.heatmap(data=retention_filled, annot=True, fmt='.0%', vmin=0.0, vmax=1.0, cmap=sns.cubehelix_palette(8))
plt.show()

